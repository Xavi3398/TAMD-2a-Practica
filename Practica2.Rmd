---
title: "Tècniques Avançades en Mineria de Dades. Pràctica 2"
author:
  - Víctor Rubert Alfonso
  - Francesc Xavier Gayà Morey
output:
  html_notebook:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

## 1. Preparació del DataFrame

Importar llibreries:

```{r, warning=FALSE}
library(unbalanced)
library(rpart)
library(class)
library(rpart.plot)
library(caret)
```

### 1.1. Lectura del CSV

Llegir el CSV amb la informació dels crims comesos a Los Ángeles:

```{r}
credit.card = read.csv2("creditcard.csv", header = TRUE, sep = ",")

# Convertir a dades numèriques
for (i in 1:(ncol(credit.card)-1)){
  credit.card[[i]] = as.numeric(credit.card[[i]])
}
credit.card$Class = factor(credit.card$Class, levels = c(0, 1))
```

### Anàlisi exploratori de les variables

```{r}
str(credit.card)
```

```{r}
head(credit.card)
```


#### Desbalanç de les classes

```{r}
n_classes = table(credit.card$Class)
print(paste("Classe 0:", n_classes[[1]], "(", round(n_classes[[1]]*100/sum(n_classes), 3), "%)"))
print(paste("Classe 1:", n_classes[[2]], "(", round(n_classes[[2]]*100/sum(n_classes), 3), "%)"))
```

#### Columna de temps
```{r}
summary(credit.card$Time)
```

```{r}
hist(credit.card$Time, breaks = 100)
```

#### Columna de quantitat
```{r}
summary(credit.card$Amount)
```

```{r}
hist(credit.card$Amount, breaks = 100)
```
Aproximació a funció power-law

#### Resta de columnes

Resum per variable:
```{r}
summary(credit.card[,2:29])
```
Resum de totes a la vegada:
```{r}
summary(c(t(credit.card[,2:29])))
```
Histograma de dues variables a mode d'exemple:
```{r}
hist(credit.card$V13, breaks = 100)
```

```{r}
hist(credit.card$V9, breaks = 100)
```

Pareix que totes les variables s'assimilen a una distribució normal, amb diferent mitjana i variança.

#### Correlació entre variables

A continuació, es representa la correlació trobada entre variables. El valor més elevat el trobam entre les variables "Amount" i V2: -0,53:

```{r, fig.width=12, fig.height=8}
library(corrplot)
corrplot(cor(credit.card[, -ncol(credit.card)]), 
                   method = "color",
                   type = "upper",
                   addCoef.col = "white",number.cex = 0.7,
                   tl.col="black", tl.srt=35,tl.cex = 0.7,
                   cl.cex = 0.7,order = "hclust")
```

Així, vegent que en general la correlació es prou baixa, consideram que no és necessari eliminar cap variable. En el cas que s'hagués trobat una alta correlació entre variables, si que hagués estat adient eliminar-ne alguna, ja que significaria que la major part de la informació que aporta una variable, pot esser explicada ja per una altra

### Normalització de les dades

Abans de procedir, és important normalitzar les dades, ja que s'hauran d'entrenar models d'aprenentatge automàtic i, com hem vist, els valors que poden prendre les diferents variables són molt diferents (la variable de temps, sobretot, pot prendre valors molt elevats).

Per fer la normalització de les dades, senzillament s'han deixat amb valor mínim de 0 i màxim de 1, reescalant totes les dades a aquest intérval:
```{r}
scl <- function(x){ (x - min(x))/(max(x) - min(x)) }
credit.norm = credit.card
credit.norm[, -ncol(credit.norm)] = data.frame(lapply(credit.card[, -ncol(credit.card)], scl))
head(credit.norm)
```

```{r}
summary(c(t(credit.norm[, -ncol(credit.norm)])))
```
Com veim, efectivament les dades es troben ara dins el rang [0,1] per a totes les variables.

### Separar en conjunts d'Entrenament i Test
```{r}
set.seed(777)
train_ind <- sample(seq_len(nrow(credit.norm)), size = floor(nrow(credit.norm)*0.8), replace = FALSE)

credit.train <- credit.norm[train_ind, ]
credit.test <- credit.norm[-train_ind, ]

nrow(credit.train)
table(credit.train$Class)
nrow(credit.test)
table(credit.test$Class)
```


## Model de classificació

```{r}
df_resultats <- function(resultats, gt, nom.tipus.tecnica, nom.tecnica, th=0.5){
  
  # Decision Tree
  arbre = rpart( Class ~ ., data = resultats)
  prediccio = predict(arbre, newdata = credit.test)
  df = df_metriques(prediccio, resultats, gt, nom.tipus.tecnica, nom.tecnica, "arbre", th)
  
  # Knn, k=3
  prediccio = knn(resultats[, -ncol(resultats)], credit.test[, -ncol(credit.test)], cl=resultats$Class, k=3)
  df = rbind(df, df_metriques(prediccio, resultats, gt, nom.tipus.tecnica, nom.tecnica, "knn", pq=TRUE, th))
  
  return(df)
}

df_metriques <- function(prediccio, resultats, gt, nom.tipus.tecnica, nom.tecnica, nom.model, pq=FALSE, th=0.5){
  if(pq)
    pred_qual = prediccio
  else
    pred_qual = get_pred_qual(prediccio, th)
  
  # Mètriques
  sensit = sensitivity(pred_qual, gt)
  specif = specificity(pred_qual, gt)
  balanced_acc = (sensit + specif) / 2
  taula_classes = table(resultats$Class)
  
  df = data.frame(
    Tipus.Tecnica = nom.tipus.tecnica,
    Tecnica = nom.tecnica,
    Model = nom.model,
    N.Pos = taula_classes[[2]],
    N.Neg = taula_classes[[1]],
    F1.Measure = F_meas(pred_qual, gt),
    Bal.Acc = balanced_acc,
    Precision = precision(pred_qual, gt),
    Recall = recall(pred_qual, gt),
    Sensitivity = sensit,
    Specificity = specif
  )
  
  return(df)
}

get_pred_qual <- function(prediccion, th=0.5){
  pred_qual=rep("0",dim(prediccion)[1])
  pred_qual[prediccion[,2]>=th]="1"
  return(as.factor(pred_qual))
}

print_metrics <- function(prediccion, gt, cm=FALSE, th=0.5){
  
  pred_qual = get_pred_qual(prediccion)
  
  if(cm)
    print(confusionMatrix(data = pred_qual, gt))
  
  else{
    pred_qual = get_pred_qual(prediccion)
    sensit = sensitivity(pred_qual, gt)
    specif = specificity(pred_qual, gt)
    balanced_acc = (sensit + specif) / 2
  
    print(paste("F1-measure:", F_meas(pred_qual, gt)))
    print(paste("Balanced Accuracy:", balanced_acc))
    print(paste("Sensitivity:", sensit))
    print(paste("Specificity:", specif))
  }
}

# Funció per ajuntar X i Y dels resultats en un sol DataFrame:
to_dataframe <- function(results) {
  new_df = results$X
  new_df$Class = results$Y
  return(new_df)
}
```


```{r}
# arbre = rpart(Class ~ ., data = credit.train)
# prediccio = predict(arbre, newdata = credit.test)
# df = df_metriques(prediccio, credit.train, credit.test$Class, "Cap", "Cap", "arbre")
# df
```

```{r}
# prediccio = knn(credit.train[, -ncol(credit.train)], credit.test[, -ncol(credit.train)], cl=credit.train$Class, k=3)
# df = df_metriques(prediccio, credit.train, credit.test$Class, "Cap", "Cap", "knn", pq=TRUE)
# df
```

```{r}
# library(neuralnet)
# ann = neuralnet(Class ~ ., data = credit.train, hidden = c(5), act.fct = "logistic", linear.output = FALSE, stepmax=1000, lifesign = "full")
# res =compute(ann, credit.test[, -ncol(credit.test)])
# res$net.result
```



```{r}
df.resultats = df_resultats(credit.train, credit.test$Class, "Cap", "Cap")
df.resultats
```

## Rebalanceig de les dades

### Undersampling

* Random
* Tomek Links
* Condensed Nearest Neighbors (CNN)
* Edited Nearest Neighbors (ENN)
* Neighborhood Cleaning Rule (NCL)
* One-Sided Detection (OSS): Tomek Links + CNN
* CNN + Tomek Links

#### Condensed Nearest Neighbors (CNN)
```{r}
results = ubCNN(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k=1)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "CNN"))
```

#### Tomek Links
```{r}
results = ubTomek(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "TL"))
```

#### One-Sided Detection (OSS): Tomek Links + CNN
```{r}
results = ubOSS(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "TL+CNN"))
```
#### Neighborhood Cleaning Rule (NCL)
```{r}
results = ubNCL(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k=3) # Amb k=10 no n'esborra cap
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "NCL"))
```

#### Edited Nearest Neighbors (ENN)
```{r}
results = ubENN(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k=3)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "ENN"))
```

#### Random
```{r}
results = ubUnder(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc = 30)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "R.Under"))
```

#### CNN + Tomek Links
```{r}
cnn2 = ubCNN(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k=1)
results = ubTomek(X=cnn2$X, Y=cnn2$Y)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "CNN+TL"))
```

#### CNN + Tomek Links + Random
```{r}
cnn2 = ubCNN(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k=1)
tl2 = ubTomek(X=cnn2$X, Y=cnn2$Y)
results = ubUnder(X=tl2$X, Y=tl2$Y, perc = 30)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "CNN+TL+R.Under"))
```

#### Random + CNN + Tomek Links
```{r}
random = ubUnder(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc = 30)
cnn2 = ubCNN(X=random$X, Y=random$Y, k=1)
results = ubTomek(X=cnn2$X, Y=cnn2$Y)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Undersampling", "R.Under+CNN+TL"))
```


### Oversampling

* Random
* SMOTE
* Borderline-SMOTE -> no està al paquet unbalanced

#### Random
```{r}
# Emprar k=0 per deixar amb els mateixos exemples que classe majoritària
results = ubOver(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, k = 300)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Oversampling", "R.Over"))
```



#### SMOTE

```{r}
results = rbind(credit.train, to_dataframe(ubSMOTE(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc.over = 20000, perc.under = 0)))
df.resultats = rbind(df.resultats, df_resultats(results, credit.test$Class, "Oversampling", "SMOTE"))
```

### Hybrid resampling

* SMOTE with random undersampling
* SMOTE + Tomek LInks
* SMOTE + ENN
* Altres combinacions

#### SMOTE with random undersampling
```{r}
results = ubSMOTE(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc.over = 20000, perc.under = 200)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Hybrid Resampling", "SMOTE+R.Under"))
```

#### SMOTE + Tomek LInks
```{r}
h.smote.tk = join.smote.results(ubSMOTE(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc.over = 20000, perc.under = 0), credit.train)
results = ubTomek(X=h.smote.tk[, -ncol(credit.train)], Y=h.smote.tk$Class)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Hybrid Resampling", "SMOTE+TK"))
```

#### SMOTE + ENN
```{r}
h.smote.enn = join.smote.results(ubSMOTE(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc.over = 20000, perc.under = 0), credit.train)
results = ubENN(X=h.smote.enn[, -ncol(credit.train)], Y=h.smote.enn$Class, k=3)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Hybrid Resampling", "SMOTE+ENN"))
```

#### SMOTE with random undersampling + Tomek Links
```{r}
h.urandom.smote.tk = ubSMOTE(X=credit.train[, -ncol(credit.train)], Y=credit.train$Class, perc.over = 2000, perc.under = 200)
results = ubTomek(X=h.urandom.smote.tk$X, Y=h.urandom.smote.tk$Y)
df.resultats = rbind(df.resultats, df_resultats(to_dataframe(results), credit.test$Class, "Hybrid Resampling", "SMOTE+R.Under+TK"))
```

### Model sensible al cost


## Resultats
```{r}
options(digits=5)
df.resultats
```











```{r}
train(Class ~ ., data = credit.train,
                   method = "knn",
                   tuneGrid = data.frame(k=3),
                   metric = "Accuracy")

```

```{r}
library(traineR)

modelo.knn <- train.knn(Class ~ ., credit.train, ks=c(3))
p <- predict(modelo.knn, credit.test, type = "class")
```
















